# First Visit Monte Carlo Method

몬테카를로 방법에서 상태에 대한 가치값을 평가하기 위한 방법 중 하나이다.

각 에피소드에서 상태 $s$ 와의 최초접촉 했을때 그 시점의 이득값을 계산해서 에피소드 별로 모두 더해서 그 상태 $s$ 의 가치를 매긴다.

### pseudo
```
input : 평가 대상인 정책 \pi
초기화 :
	모든 s \in S에 대해 임의의 값으로 V(s) \in \R을 초기화
	모든 s \in S에 대해 값이 채워지지 않은 리스트를 Returns(s)변수에 대입
	
	무한루프:
		정책 \pi를 따르는 에피소드 생성 : S0,A0,R1,S1,A1,R1,......,RT
		변수 G에 0을 대입
		에피소드의 각 단계에 대해 반복수행, t=T-1, T-2, ..... , 0:
			변수 G에 \gamma G+R_{t+1}을 대입
			S_t가 S0,S1,...,S_{t-1}안에 나타나지 않는다면:  ##해당 조건은 현재 에피소드에서 S_t를 최초접촉 한 것인지를 판단하기 위한 조건문
				리스트 Returns(S_t)에 변수 G를 새 항목으로 추가
				리스트 Returns(S_t)에 대한 평균 V(S_t)에 대입.  ##에피소드를 반복하면서 V(s_t)값을 계속 업데이트 해주면서 참값에 근접시킨다.
			

##최초 접촉 mc 방법 
	-> 각 상태 s_t에 대해 각 에피소드에서 처음 접촉했을때 발생하는 보상 값의 평균을 계산하여서 해당 상태에 대한 target policy의 가치를 측정.
```
